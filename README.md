# **Deep Learning Classics: Read, Review, Recode**

The most impactful deep learning research papers **analyzed, summarized**, and **re-implemented from scratch** in PyTorch for educational purposes.

## Goals
- Understand and explain seminal deep learning papers
- Reproduce their key ideas with clean and minimal code
- Categorize by architecture or task.


## Paper List & Implementation Status

### üèóÔ∏è Convolutional Neural Networks
- [ ] [LeNet-5 (1998)](ConvNets/LeNet5)
- [ ] [AlexNet (2012)](ConvNets/AlexNet)
- [ ] [VGG (2014)](ConvNets/VGG)
- [ ] [GoogLeNet / Inception (2015)](ConvNets/Inception)
- [ ] [ResNet (2015)](ConvNets/ResNet) ‚åõÔ∏è
- [ ] [DenseNet (2016)](ConvNets/DenseNet)
- [ ] [You Only Look Once (YOLO) (2016)](ConvNets/YOLO) ‚åõÔ∏è


### üîÅ Sequence Models
- [ ] [Vanilla RNNs](SequenceModels/RNN)
- [ ] [LSTM (1997)](SequenceModels/LSTM)
- [ ] [GRU (2014)](SequenceModels/GRU)


### üîÆ Transformers & Attention
- [ ] [Transformer (Vaswani et al. 2017)](Transformers/Transformer)
- [ ] [BERT (2018)](Transformers/BERT)
- [ ] [GPT (2018)](Transformers/GPT)
- [ ] [Vision Transformer (ViT, 2020)](Transformers/ViT)
- [ ] [Swin Transformer (2021)](Transformers/Swin)


## Structure of Each Paper
Every paper folder includes:
- `README.md`: Summary and insights from the paper
- `paper_name.pdf` *(if publicly available)*
- `implementation.py`: From-scratch implementation
- `notebook.ipynb`: Minimal example/demo (if applicable)
